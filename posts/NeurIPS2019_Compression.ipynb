{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeurIPS2019_Compression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "nikola": {
      "category": "notes",
      "date": "2020-01-07 20:00:00 UTC-08:00",
      "description": "Conference review for model compression presented in NeurIPS 2019",
      "link": "",
      "slug": "neurips2019",
      "tags": "notes,compression,neurips",
      "title": "Review of NeurIPS 2019 for Model Compression",
      "type": "text"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slhRYJbZSzQP",
        "colab_type": "text"
      },
      "source": [
        "# Review of NeurIPS 2019 for Model Compression\n",
        "\n",
        "## Sessions\n",
        "\n",
        "+ [Deep Learning -- Efficient Inference Methods](https://nips.cc/Conferences/2019/Schedule?showParentSession=15511)\n",
        "\n",
        "+ [Workshop on Energy Efficient Machine Learning and Cognitive Computing](https://www.emc2-workshop.com/neurips-19)\n",
        "\n",
        "## Workshop keynotes\n",
	"<!-- TEASER_END -->\n",
        "\n",
        "+ [Efficient Computing for AI and Robotics](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-sze-keynote.pdf)\n",
        "+ [Putting the “Machine” Back in Machine Learning: The Case for Hardware-ML Model Co-design](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-marculescu-talk.pdf)\n",
        "+ [Cheap, Fast, and Low Power Deep Learning: I need it now!](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-delp-keynote.pdf)\n",
        "+ [Advances and Prospects for In-memory Computing](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-verma-talk.pdf)\n",
        "+ [Algorithm-Accelerator Co-Design for Neural Network Specialization](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-zhang-talk.pdf)\n",
        "+ [Adaptive Multi-Task Neural Networks for Efficient Inference](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-feris-talk.pdf)\n",
        "\n",
        "## Pruning\n",
        "\n",
        "+ [Channel Gating Neural Networks](http://papers.nips.cc/paper/8464-channel-gating-neural-networks): proposes an effective way to create dynamically pruned network for inference by channel gating. It provides a clear instruction to design the network architecture and to train such network. The inference stage requires carefully implemented gating function to reduce the computation (therefore better efficiency).\n",
        "\n",
        "+ [AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters](http://papers.nips.cc/paper/9521-autoprune-automatic-network-pruning-by-regularizing-auxiliary-parameters): trains an additional set of parameters to prune the underlying network weights (similar to gating).\n",
        "\n",
        "+ [Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks](http://papers.nips.cc/paper/8486-gate-decorator-global-filter-pruning-method-for-accelerating-deep-convolutional-neural-networks): provides a detailed solution for effective channel pruning.  Using a trainable weight factor to guide pruning has been studied in [Liu et al. 2017](http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf).  Using Taylor expansion to predict the importance has been explored in [Molchanov et al. 2016](https://arxiv.org/abs/1611.06440).  This paper combined the techniques together along with tweaks on skip connection and batch normalization to form a concrete solution for filter/channel pruning.\n",
        "\n",
        "+ [Network Pruning via Transformable Architecture Search](http://papers.nips.cc/paper/8364-network-pruning-via-transformable-architecture-search): searches for the best width and depth using NAS.  It is not a typical NAS, instead, it represents the network depth and width probabilistically and learn them gradually via validation set to form the final pruned model with distillation.\n",
        "\n",
        "## Low rank\n",
        "\n",
        "+ [Singleshot : a scalable Tucker tensor decomposition](http://papers.nips.cc/paper/8860-singleshot-a-scalable-tucker-tensor-decomposition): proposes efficient algorithms for Tucker decomposition.  The decomposition can improve model inference speed and reduce model size.\n",
        "\n",
        "+ [[w]Trained Rank Pruning for Efficient Deep Neural Networks](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-15.pdf): applies SGD on decomposed matrices periodically to obtain a low-rank network. \n",
        "\n",
        "+ [[w] Pushing the limits of RNN Compression](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-13.pdf): uses Kronecker products to decompose matrices in RNN layers and obtains 15x or more compression.\n",
        "\n",
        "## Quantization\n",
        "\n",
        "+ [Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks](http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks): proposes a new float 8 representation to effectively quantize DNN.\n",
        "\n",
        "+ [The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic](http://papers.nips.cc/paper/9052-the-synthesis-of-xnor-recurrent-neural-networks-with-stochastic-logic): fully adopts XNOR operations for the LSTM-based RNN model.\n",
        "\n",
        "+ [MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization](http://papers.nips.cc/paper/8647-metaquant-learning-to-quantize-by-learning-to-penetrate-non-differentiable-quantization): learns the mapping for gradient update for quantized model instead of using a pre-defined schema (e.g. straight-through-estimator).\n",
        "\n",
        "+ [A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off](http://papers.nips.cc/paper/8926-a-mean-field-theory-of-quantized-deep-networks-the-quantization-depth-trade-off): provides a guidance to balance the activation quantization (N) and the depth (L) of a model, $L\\propto N^{1.87}$.\n",
        "\n",
        "+ [[w]Progressive Stochastic Binarization of Deep Networks](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-23.pdf): uses binary weights stochastically to present float weights in a statistically sounding approach.\n",
        "\n",
        "+ [[w] Regularized Binary Network Training](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-29.pdf): improves the training procedure for binary network.\n",
        "\n",
        "+ [[w] Neural Networks Weights Quantization: Target None-retraining Ternary (TNT)](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-16.pdf): uses cosine similarity to convert network weights into tenary values.\n",
        "\n",
        "+ [[w] Instant Quantization of Neural Networks using Monte Carlo Methods](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-17.pdf): proposes to quantize model weights by Monte Carlo sampling to reduce retraining.  The model size is controlled by the number of samples.\n",
        "\n",
        "+ [[w] Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Inference](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-11.pdf): provides a method to estimate the optimal quantization mapping and to efficiently finetune the quantized model.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "+ [CondConv: Conditionally Parameterized Convolutions for Efficient Inference](http://papers.nips.cc/paper/8412-condconv-conditionally-parameterized-convolutions-for-efficient-inference): learns the dynamic (input-dependent) weights to combine different kernels for convolutions. It shows better accuracy/speed on mobilenet models.\n",
        "\n",
        "+ [More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation](http://papers.nips.cc/paper/8498-more-is-less-learning-efficient-video-representations-by-big-little-network-and-depthwise-temporal-aggregation)\n",
        "\n",
        "+ [SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers](http://papers.nips.cc/paper/8743-sparse-sparse-architecture-search-for-cnns-on-resource-constrained-microcontrollers): demonstrates (first time?) to apply CNN architecture with dozen KB memory and model size.\n",
        "\n",
        "+ [Constrained deep neural network architecture search for IoT devices accounting for hardware calibration](http://papers.nips.cc/paper/8838-constrained-deep-neural-network-architecture-search-for-iot-devices-accounting-for-hardware-calibration): applies genetic evolution to mutate neural architecture (mainly number of neurons).  It also includes realistic measure of inference time for the resource constraint.\n",
        "\n",
        "+ [Einconv: Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks](http://papers.nips.cc/paper/8793-exploring-unexplored-tensor-network-decompositions-for-convolutional-neural-networks): provides an interesting view to decompose the convolustion as tensor network and leverage it to search for efficient neural architecture.\n",
        "\n",
        "+ [[w]AutoSlim: Towards One-Shot Architecture Search for Channel Numbers](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-2.pdf): searches slim network (subnet) during greedy training process to find the best model.\n",
        "\n",
        "+ [[w] Energy-Aware Neural Architecture Optimization With Splitting Steepest Descent](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-39.pdf): proposes to split neurons along its steepest descent direction to grow neural network (NAS).  It provides a theoretical sounding procedure to assign new neurons after splitting.\n",
        "\n",
        "## Hardware-related\n",
        "\n",
        "+ [[w] Exploring Bit-Slice Sparsity in Deep Neural Networks for Efficient ReRAM-Based Deployment](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-12.pdf): proposes an l1 norm for ReRAM-based device for efficient computation.\n",
        "\n",
        "+ [[w] Improving Efficiency in Neural Network Accelerator using Operands Hamming Distance Optimization](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-30.pdf): reduces the energy consumption by regularizing the datapath with the proposed operands hamming distance optimization.\n",
        "\n",
        "## Others\n",
        "\n",
        "- [Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask](http://papers.nips.cc/paper/8618-deconstructing-lottery-tickets-zeros-signs-and-the-supermask): illustrates the importance of mask strategy and provide a better way to learn the mask (for training pruned model).\n",
        "\n",
        "- [Model Compression with Adversarial Robustness: A Unified Optimization Framework](http://papers.nips.cc/paper/8410-model-compression-with-adversarial-robustness-a-unified-optimization-framework): proposes adversarial attack-aware model compression.\n",
        "\n",
        "- [[w] On hardware-aware probabilistic frameworks for resource constrained embedded applications](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-38.pdf): targets compression for probabilistic models.\n",
        "\n",
        "- [[w] Doubly Sparse: Sparse Mixture of Sparse Experts for Efficient Softmax Inference](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-9.pdf): speeds up the search for top-k classes for softmax layer.\n",
        "\n",
        "- [[w] Algorithm-hardware Co-design for Deformable Convolution](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-10.pdf): improves the deformable convolution by rounding, bounding and squared shape for efficient inference on FPGA devices.\n",
        "\n",
        "## Application\n",
        "\n",
        "+ [Point-Voxel CNN for Efficient 3D Deep Learning](http://papers.nips.cc/paper/8382-point-voxel-cnn-for-efficient-3d-deep-learning): solves the efficiency challenges of applying DL to  3D point cloud applications.\n",
        "+ [LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition](http://papers.nips.cc/paper/8993-liteeval-a-coarse-to-fine-framework-for-resource-efficient-video-recognition): uses two LSTM networks (fine/coarse) for video recognition.  The coarse network is always-on, whereas the fine network is activated by the output of the hidden layers in the coarse network.  The gates control the activation is a trainable 1-layer dense layer with probabilistic nature.  Also, the hidden information is shared between the coarse and fine networks.\n",
        "+ [[w]YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-1.pdf): demonstrates a highly efficient YOLO implementation.\n",
        "+ [[w]Q8BERT: Quantized 8Bit BERT](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-31.pdf): applies 8bit quantization to deep language models, such as BERT.\n",
        "+ [[w] Training Compact Models for Low Resource Entity Tagging using Pre-trained Language Models](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-25.pdf): distills transformer model to train compact model.\n",
        "+ [[w] Fully Quantized Transformer for Improved Translation](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-36.pdf): applies quantization to transformer network for language translation.\n",
        "+ [[w] Spoken Language Understanding on the Edge](https://www.emc2-workshop.com/assets/docs/neurips-19/emc2-neurips19-paper-19.pdf): presents the system design of language understanding device for edge applications.\n"
      ]
    }
  ]
}